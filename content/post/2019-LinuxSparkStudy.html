---
title: Linux Spark 掉坑逃坑记
author: 小冠
date: '2019-05-11'
slug: 2019-LinuxSparkStudy
categories:
  - 我码故我在
tags:
  - spark
  - linux
banner: 'banners/Apache-Spark.jpg'
description: '安装、配置、端口等问题和解决方法'
images: []
menu: ''
---



<!--more-->
<div class="section level2">
<h2>1 安装软件</h2>
<p>Linux下安装软件于windows有很大不同。大致来讲，有4种方式，有的地方会更加的方便，但有的地方也非常的不方便。</p>
<div id="linux" class="section level3">
<h3>1.1 linux下安装软件的四种方法。</h3>
<ul>
<li><p>第1种，在线用命令安装，非常方便，敲一行命令，就可以直接安装一个软件，网上的攻略也是经常是用这种方式，pip install SomePackage</p></li>
<li><p>第2种，下载预编译包，这大致相当于windows下的绿色软件，下载的软件包直接解压，再修改配置文件，设置环境变量，就好了。</p></li>
<li><p>第3种，sh文件，这是类似与windows下如exe安装包，只要下载好，用命令安执行，按照提示一步一步操作就可以了，只是把用鼠标点击一步步安装，改成了敲击键盘和回车。</p></li>
<li><p>第四种，最复杂如，下载软件的源代码，解压后自己编译，不但要使用编译命令，还必须要有编译的环境，一般是gcc。</p></li>
<li><p>我采用的方法。 第一种方法虽然好，但我们无法使用，因为我们的服务器与外网隔离。用的最多的是第二种，先用自己电脑把压缩包下载好，查杀病毒，上传到服务器，解压再配置文件。虽然看上去还算简单，但Linux下压缩包格式有很多种，有tar、gz等等，不同的文件要用不同的解压命令，几乎是没有办法记住，只能现用现查。</p></li>
</ul>
</div>
<div class="section level3">
<h3>1.2 安装步骤</h3>
<div id="java" class="section level4">
<h4>1.2.1 安装java，第一个坑</h4>
<p>前面的步骤一样，我下载如记rpm包，安装本省挺顺利。 修改配置文件是很大的坑，千万不能出错，用这个命令打开配置文件</p>
<pre class="r"><code>vi /etc/profile`</code></pre>
<p>修改这个文件，配置环境变量，java需要配置好几个环境变量，<strong>PATH，CLASSPATH，JAVA_HOME</strong>， 这个配置文件，环境变量命令如下：</p>
<pre class="r"><code>export JAVA_HOME=/usr/share/jdk1.6.0_14&#39;
export PATH=\$JAVA_HOME/bin:$PATH</code></pre>
<p>每个变量用** : <strong>隔开，而windows是用</strong> ; <strong>, 连接用</strong> $ **，千万不能出错</p>
<p>我第一次就把冒号输入成了分号，导致把整个PATH都没有用了，几乎所有命令都用不了 vi命令也用不了，所以也没有办法打开配置文件修改，真是难题，这记第一根坑</p>
<blockquote>
<p>解决，PATH没有了，直接打命令找不到，但命令本身还是在的，只要的文件夹种找到，还是可以用如， 在各个文件夹中找到了vi，切换到文件夹中后，重新修改。但是修改后的source命令到处都没有找到， 好在只要重启，就可以自动执行，继续找shutdown命令，然后用shutdown restart now。 重启后终于好了。</p>
</blockquote>
</div>
<div id="scala" class="section level4">
<h4>1.2.2 安装scala</h4>
<p>也用rpm包，一切顺利</p>
</div>
<div id="hadoop" class="section level4">
<h4>1.2.3 安装Hadoop</h4>
<p>tar.gz包，下载了2.7的，一切顺利，但暂时用不到，我就没有去配置。</p>
</div>
<div id="spark" class="section level4">
<h4>1.2.4 安装Spark</h4>
<p>终于到正题了，下载了spark-2.4.0-bin-hadoop2.7.tgz，貌似自带hadoop，不过我也没时间试过了。</p>
</div>
</div>
</div>
<div class="section level2">
<h2>2 应用篇</h2>
<div class="section level3">
<h3>2.1 端口篇</h3>
<p>粗粗查了查，spark要用到8080，8081，7077，4040，联系运维开放端口。 又掉小坑，错看成7070，不过没关系，改一下配置应该就可以解决。</p>
<blockquote>
<p>解决: 只要输入命令时指定端口就OK了，不用改配置文件也可以</p>
</blockquote>
<pre class="r"><code>SPARK_MASTER_PORT=7070 sbin/start-all.sh</code></pre>
<p>访问ip:8080， 显示spark在7070端口运行</p>
</div>
<div id="spark-shell" class="section level3">
<h3>2.2 试用spark-shell</h3>
<p>安装完成后，敲击命令spark-shell就可以出现这样的界面，说明没问题了。 因为spark是用scala写的，所以spark的shell是scala语言的。</p>
</div>
<div id="pyspark" class="section level3">
<h3>2.3 试用pyspark</h3>
<p>我主要还是想用python写spark的代码，所以要用pyspark，这个装好spark也都是自带的。 但敲入命令pyspark后提示错，。</p>
</div>
<div class="section level3">
<h3>2.4 大坑——版本问题</h3>
<p>经过反复测试和搜寻，包括下载安装，终于确定这个错误的原因，应该的是python的版本问题。 我的系统是CentOS6.5，默认的Python是2.6，似乎Pyspark并不支持2.6。 所以接下来，我要几条路可走，1升级Python到2.7；2用Anaconda直接升级到3.6；3重新环系统。</p>
<blockquote>
<p>解决:第3条路肯定是放在最后了，考虑到pyspark用2.7的人多，优先选择这第一条路， 但实际上很难，Python2.7就属于前面讲到安装软件的第四种，需要下载源代码，自行编译。 测试，CentOS6.5没有默认安装编译器，无网安装编译器很复杂。 所以不得不考虑第2条路，虽然有风险献，Python3的特性有的可能Pyspark不支持，先这样吧。 下载Anaconda，这个就是软件安装的第3种——sh文件，执行后安提示安装，顺利。 然后再次修改配置，配置环境变量，并把新的Python环境设为系统默认。 听说升级为Python3后，系统的Yam就用不了了，不过目前还没发现异常。 重启，再次敲pyspark顺利出现Spark界面，终于搞定可以开工了。</p>
</blockquote>
</div>
<div id="jupyter" class="section level3">
<h3>2.5 Jupyter</h3>
<p>既然安装了Jupyter，当然想用jupyter notebook作为开发环境， 这样我也不用频繁登录服务器了，可以直接在自己电脑上登录服务器的jupyter notebook写和测试代码了。 Anaconda3都是自带jupyter的，找到配置文件，修改密码，端口还是默认的8888，修改为root可用。</p>
<pre class="r"><code>c.NotebookApp.allow_root = True</code></pre>
<p>之后需要设置密码，方法是先用Python生成密匙的SHA1散列码，</p>
<pre class="r"><code>from notebook.auth import passwd
passwd()</code></pre>
<p>输入密匙后将SHA1码复制到设置文件中，</p>
<pre class="r"><code>c.NotebookApp.password = u&#39;sha1:&#39;</code></pre>
<p>jupyter notebook，提示已部署，但在自己电脑上就是上不了。</p>
<blockquote>
<p>解决: 不断与运维确认端口后，也用iptables查看了，端口确实开放， 再次搜索，发现Jupyter的配置文件中还有个ip控制的设置， 将允许访问ip改为星号, 也要修改</p>
</blockquote>
<pre class="r"><code>c.NotebookApp.ip = &#39;*&#39;
c.NotebookApp.allow_remote_access = True</code></pre>
<p>再次启动，在自己电脑上访问，成功！</p>
<p>还有后续, 介绍学写spark代码中的收获</p>
</div>
</div>
